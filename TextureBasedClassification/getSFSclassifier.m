% created by Lynda from trainmyClassifier.m by Jakob Nikolas Kather 2015 - 2016
% license: see separate LICENSE file, includes disclaimer
function [ modeltesty ] = ...
    getSFSclassifier(xtr,ytr,xte, classifMethod)


alldata=xtr;
alldata(:, end+1)=ytr;
switch lower(classifMethod)
%     % --------------- NEURAL
     case 'neural'
%     % NOTE: the dataset could also be classified by a neural network. When 
%     % using the MATLAB GUI nprtool, a well-performing neural network can be
%     % easily trained and classification accuracy is very high (comparable
%     % to SVM). However, this has not yet been implemented here.
%     
%     [trainedClassifier, validationAccuracy,ConfMat, ROCraw] = ...
%      trainMyNetwork(DataIn,NcrossVal);
      disp('no neural option')
      
     otherwise
%     % --------------- OTHER THAN NEURAL 
     numFeat = size(alldata,2) - 1;
     numResp = 1; % no. of features and response
     classNames=unique(ytr);
    % Convert input to table
    DataIn = table(alldata); 
    DataIn.Properties.VariableNames = {'column'};

    % prepare column names
    nameMat = 'column_1#';
    for i=2:((numFeat+numResp)) 
        nameMat = [nameMat,['#column_',num2str(i)]]; 
    end
    colnames = strsplit(nameMat,'#');

    % Split matrices in the input table into vectors
    DataIn = [DataIn(:,setdiff(DataIn.Properties.VariableNames, ...
        {'column'})), array2table(table2array(DataIn(:,{'column'})), ...
        'VariableNames', colnames)];

    % Extract predictors and response, convert to arrays
    predictorNames = colnames(1:(end-1));    responseName = colnames(end);
    xtr = DataIn(:,predictorNames);   ytr = DataIn(:,responseName);
        xtr = table2array(varfun(@double, xtr));
        ytr = table2array(varfun(@double, ytr));
        
%    disp('start training...');
    switch lower(classifMethod)
        
        % --------------- support vector machine (SVM)
        case {'rbfsvm','linsvm', 'polysvm'}
            costmat=[0,1,1,2,1,2;1,0,1,1,3,3;1,1,0,1,2,2;1,1,1,0,1,3;1,3,3,1,0,2;1,3,1,2,3,0];
            rng(2);
            switch lower(classifMethod)
                case 'polysvm' % radial basis function SVM
                    template = templateSVM('KernelFunction', 'polynomial', 'PolynomialOrder', ...
                        2, 'KernelScale', 'auto', 'BoxConstraint', 1, 'Standardize', 1);
                case 'rbfsvm' % radial basis function SVM
                    template = templateSVM('KernelFunction', 'rbf', 'PolynomialOrder', ...
                        [], 'KernelScale', 'auto', 'BoxConstraint', 1, 'Standardize', 1);
                case 'linsvm' % linear SVM
                    template = templateSVM('KernelFunction', 'linear', 'PolynomialOrder', ...
                        [], 'KernelScale', 'auto', 'BoxConstraint', 1, 'Standardize', 1);
            end % end svm subtypes, start svm common part
            trainedClassifier = fitcecoc(xtr, ytr, ...
                'Learners', template, 'Coding', 'onevsone',...
                'PredictorNames', predictorNames, 'ResponseName', ...
                char(responseName), 'ClassNames', classNames, ...
                'Cost', costmat, 'Prior', [.5749, .08915, .03439, .12187, .05772, .11246]);
            % --------------- ensemble of decision trees
        case {'rusbtree', 'adabtree', 'subspacetree', 'bagtree', 'lsbtree'}
            switch lower(classifMethod)
                case 'rusbtree'
                    template = templateTree(...
                        'MaxNumSplits', 20);
                    trainedClassifier = fitensemble(...
                        xtr, ...
                        ytr, ...
                        'RUSBoost', ...
                        30, ...
                        template, ...
                        'Type', 'Classification', ...
                        'LearnRate', 0.1, ...
                        'ClassNames', classNames);
                case 'adabtree'
                    template = templateTree(...
                        'MaxNumSplits', 20);
                    trainedClassifier = fitensemble(...
                        xtr, ...
                        ytr, ...
                        'AdaBoostM2', ...
                        30, ...
                        template, ...
                        'Type', 'Classification', ...
                        'LearnRate', 0.1, ...
                        'ClassNames', classNames);
                case 'subspacetree'
                    template = templateDiscriminant(...
                        'DiscrimType', 'quadratic');
                    trainedClassifier = fitensemble(...
                        xtr, ...
                        ytr, ...
                        'Subspace', ...
                        30, ...
                        template, ...
                        'Type', 'Classification', ...
                        'ClassNames', classNames);
                case 'bagtree'
                    template = templateTree(...
                        'MaxNumSplits', 20);
                    trainedClassifier = fitensemble(...
                        xtr, ...
                        ytr, ...
                        'Bag', ...
                        30, ...
                        template, ...
                        'Type', 'Classification', ...
                        'ClassNames', classNames);
                case 'lsbtree'
                    template = templateTree(...
                        'MaxNumSplits', 20);
                    trainedClassifier = fitensemble(...
                        xtr, ...
                        ytr, ...
                        'LSBoost', ...
                        30, ...
                        template, ...
                        'Type', 'Classification', ...
                        'LearnRate', 0.1, ...
                        'ClassNames', classNames);
            end % end case of ensembletree
            
        % --------------- 1-nearest neighbor (1-NN)
        case '1nn'
            trainedClassifier = fitcknn(xtr, ytr, 'PredictorNames',...
                predictorNames, 'ResponseName', char(responseName), 'ClassNames', ...
                classNames, 'Distance', 'Euclidean', 'Exponent', '',...
                'NumNeighbors', 1, 'DistanceWeight', 'Equal', 'StandardizeData', 1);
        case 'optnn'
            trainedClassifier = fitcknn(xtr,ytr,'OptimizeHyperparameters','all',... 
                'HyperparameterOptimizationOptions',... 
                struct('AcquisitionFunctionName','expected-improvement-plus', 'ShowPlots', false, 'Verbose', 0, 'UseParallel', false, 'SaveIntermediateResults',true));
            
        % --------------- Naive Bayes 
        case 'naivebayes'
            distNames=repmat({'kernel'}, 1, length(predictorNames));
            trainedClassifier= fitcnb(xtr, ytr,'PredictorNames',...
                predictorNames, 'ResponseName', char(responseName),'Distribution',...
                 distNames, 'ClassNames', ...
                classNames);
            
    end % end svm or not svm
    
    % ------ all non-neural methods continuing here
    % Perform cross-validation = re-train and test the classifier K times
%     disp('start cross validation...');
%     partitionedModel = crossval(trainedClassifier, 'KFold', NcrossVal);
%     disp('properties of partitioned set for cross validation'); partitionedModel.Partition
%     % Compute validation accuracy on partitioned model
%     disp('start validation...'); 
%     validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');
%         %accuracy is computed by "classiferror", which is weighted fraction
%         %of misclassifications
%     % Compute validation predictions and scores
%     disp('computing validation predictions and scores...');
%     [validationPredictions, validationScores] = kfoldPredict(partitionedModel);
%     ConfMat = confusionmat(ytr,validationPredictions);
%     % Prepare data for ROC curves (reformat arrays)
%     trues = zeros(numel(unique(ytr)),size(ytr,1));
%     for i = 1:numel(unique(ytr)), trues(i,ytr==i) = 1; end
%     ROCraw.true = trues; ROCraw.predicted = validationScores;
    modeltesty=predict(trainedClassifier,xte);
%     
%end % end neural or not neural
end % end function